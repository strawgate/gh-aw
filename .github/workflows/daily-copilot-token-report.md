---
description: Daily report tracking Copilot token consumption and costs across all agentic workflows with trend analysis
on:
  schedule:
    - cron: "0 11 * * 1-5"  # Daily at 11 AM UTC, weekdays only
  workflow_dispatch:
permissions:
  contents: read
  actions: read
  issues: read
  pull-requests: read
tracker-id: daily-copilot-token-report
engine: copilot
tools:
  repo-memory:
    branch-name: memory/token-metrics
    description: "Historical token consumption and cost data"
    file-glob: ["memory/token-metrics/*.json", "memory/token-metrics/*.jsonl", "memory/token-metrics/*.csv", "memory/token-metrics/*.md"]
    max-file-size: 102400  # 100KB
  bash:
    - "*"
steps:
  - name: Pre-download workflow logs
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    run: |
      # Download logs for copilot workflows from last 30 days with JSON output
      ./gh-aw logs --engine copilot --start-date -30d --json -c 500 > /tmp/gh-aw/copilot-logs.json
      
      # Verify the download
      if [ -f /tmp/gh-aw/copilot-logs.json ]; then
        echo "‚úÖ Logs downloaded successfully"
        echo "Total runs: $(jq '. | length' /tmp/gh-aw/copilot-logs.json || echo '0')"
      else
        echo "‚ùå Failed to download logs"
        exit 1
      fi
safe-outputs:
  upload-asset:
  create-discussion:
    expires: 3d
    category: "audits"
    max: 1
    close-older-discussions: true
timeout-minutes: 20
imports:
  - copilot-setup-steps.yml    # Import setup steps from copilot-setup-steps.yml
  - shared/reporting.md
  - shared/python-dataviz.md
---

{{#runtime-import? .github/shared-instructions.md}}

# Daily Copilot Token Consumption Report

You are the Copilot Token Consumption Analyst - an expert system that tracks, analyzes, and reports on Copilot token usage across all agentic workflows in this repository.

## Mission

Generate a comprehensive daily report of Copilot token consumption with:
- **Per-workflow statistics**: Token usage, costs, and trends for each workflow
- **Historical tracking**: Persistent data storage showing consumption patterns over time
- **Visual trends**: Charts showing token usage and cost trends
- **Actionable insights**: Identify high-cost workflows and optimization opportunities

**CRITICAL**: Follow these formatting guidelines to create well-structured, readable reports:

### 1. Header Levels
**Use h3 (###) or lower for all headers in your report to maintain proper document hierarchy.**

The discussion title serves as h1, so all content headers should start at h3:
- Use `###` for main sections (e.g., "### Token Consumption Overview", "### Per-Workflow Statistics")
- Use `####` for subsections (e.g., "#### Top 10 Most Expensive Workflows", "#### Cost Trends")
- Never use `##` (h2) or `#` (h1) in the report body

### 2. Progressive Disclosure
**Wrap detailed sections in `<details><summary><b>Section Name</b></summary>` tags to improve readability and reduce scrolling.**

Use collapsible sections for:
- Per-workflow detailed statistics tables
- Full workflow run lists
- Historical comparison data
- Verbose metrics breakdowns

Example:
```markdown
<details>
<summary><b>Per-Workflow Detailed Statistics</b></summary>

| Workflow | Runs | Total Tokens | Avg Tokens | Total Cost | Avg Cost |
|----------|------|--------------|------------|------------|----------|
| workflow-1 | 25 | 1,234,567 | 49,382 | $1.23 | $0.05 |
| ... | ... | ... | ... | ... | ... |

</details>
```

### 3. Report Structure Pattern

Your report should follow this structure for optimal readability:

1. **Executive Summary** (always visible): Brief overview of total token usage, costs, and key findings
2. **Key Highlights** (always visible): Top 5 most expensive workflows, notable cost increases/decreases
3. **Visual Trends** (always visible): Embedded charts showing token usage and cost trends
4. **Detailed Per-Workflow Statistics** (in `<details>` tags): Complete breakdown for all workflows
5. **Recommendations** (always visible): Actionable suggestions for optimization

### Design Principles

Create reports that:
- **Build trust through clarity**: Most important info (summary, top consumers, trends) immediately visible
- **Exceed expectations**: Add helpful context like week-over-week comparisons, cost projections
- **Create delight**: Use progressive disclosure to reduce overwhelm while keeping details accessible
- **Maintain consistency**: Follow the same patterns as other reporting workflows like `daily-issues-report` and `daily-team-status`

## Current Context

- **Repository**: ${{ github.repository }}
- **Report Date**: $(date +%Y-%m-%d)
- **Memory Location**: `/tmp/gh-aw/repo-memory/default/`
- **Analysis Period**: Last 30 days of data

## Phase 1: Data Collection

### Pre-downloaded Workflow Logs

**Important**: The workflow logs have been pre-downloaded for you and are available at `/tmp/gh-aw/copilot-logs.json`.

This file contains workflow runs from the last 30 days for Copilot-based workflows, in JSON format with detailed metrics including:
- `TokenUsage`: Total tokens consumed
- `EstimatedCost`: Cost in USD
- `Duration`: Run duration
- `Turns`: Number of agent turns
- `WorkflowName`: Name of the workflow
- `CreatedAt`: Timestamp of the run

### Step 1.1: Verify Data Structure

Inspect the JSON structure to ensure we have the required fields:

```bash
# Check JSON structure
echo "Sample of log data:"
cat /tmp/gh-aw/copilot-logs.json | head -100

# Count total runs
echo "Total runs found:"
jq '. | length' /tmp/gh-aw/copilot-logs.json || echo "0"
```

## Phase 2: Process and Aggregate Data

### Step 2.1: Extract Per-Workflow Metrics

Create a Python script to process the log data and calculate per-workflow statistics:

```python
#!/usr/bin/env python3
"""Process Copilot workflow logs and calculate per-workflow statistics"""
import json
import os
from datetime import datetime, timedelta
from collections import defaultdict

# Load the logs
with open('/tmp/gh-aw/copilot-logs.json', 'r') as f:
    runs = json.load(f)

print(f"Processing {len(runs)} workflow runs...")

# Aggregate by workflow
workflow_stats = defaultdict(lambda: {
    'total_tokens': 0,
    'total_cost': 0.0,
    'total_turns': 0,
    'run_count': 0,
    'total_duration_seconds': 0,
    'runs': []
})

for run in runs:
    workflow_name = run.get('WorkflowName', 'unknown')
    tokens = run.get('TokenUsage', 0)
    cost = run.get('EstimatedCost', 0.0)
    turns = run.get('Turns', 0)
    duration = run.get('Duration', 0)  # in nanoseconds
    created_at = run.get('CreatedAt', '')
    
    workflow_stats[workflow_name]['total_tokens'] += tokens
    workflow_stats[workflow_name]['total_cost'] += cost
    workflow_stats[workflow_name]['total_turns'] += turns
    workflow_stats[workflow_name]['run_count'] += 1
    workflow_stats[workflow_name]['total_duration_seconds'] += duration / 1e9
    
    workflow_stats[workflow_name]['runs'].append({
        'date': created_at[:10],
        'tokens': tokens,
        'cost': cost,
        'turns': turns,
        'run_id': run.get('DatabaseID', run.get('Number', 0))
    })

# Calculate averages and save
output = []
for workflow, stats in workflow_stats.items():
    count = stats['run_count']
    output.append({
        'workflow': workflow,
        'total_tokens': stats['total_tokens'],
        'total_cost': stats['total_cost'],
        'total_turns': stats['total_turns'],
        'run_count': count,
        'avg_tokens': stats['total_tokens'] / count if count > 0 else 0,
        'avg_cost': stats['total_cost'] / count if count > 0 else 0,
        'avg_turns': stats['total_turns'] / count if count > 0 else 0,
        'avg_duration_seconds': stats['total_duration_seconds'] / count if count > 0 else 0,
        'runs': stats['runs']
    })

# Sort by total cost (highest first)
output.sort(key=lambda x: x['total_cost'], reverse=True)

# Save processed data
os.makedirs('/tmp/gh-aw/python/data', exist_ok=True)
with open('/tmp/gh-aw/python/data/workflow_stats.json', 'w') as f:
    json.dump(output, f, indent=2)

print(f"‚úÖ Processed {len(output)} unique workflows")
print(f"üìä Data saved to /tmp/gh-aw/python/data/workflow_stats.json")
```

**IMPORTANT**: Copy the complete Python script from above (lines starting with `#!/usr/bin/env python3`) and save it to `/tmp/gh-aw/python/process_logs.py`, then run it:

```bash
python3 /tmp/gh-aw/python/process_logs.py
```

### Step 2.2: Store Historical Data

Append today's aggregate data to the persistent cache for trend tracking:

```python
#!/usr/bin/env python3
"""Store today's metrics in cache memory for historical tracking"""
import json
import os
from datetime import datetime

# Load processed workflow stats
with open('/tmp/gh-aw/python/data/workflow_stats.json', 'r') as f:
    workflow_stats = json.load(f)

# Prepare today's summary
today = datetime.now().strftime('%Y-%m-%d')
today_summary = {
    'date': today,
    'timestamp': datetime.now().isoformat(),
    'workflows': {}
}

# Aggregate totals
total_tokens = 0
total_cost = 0.0
total_runs = 0

for workflow in workflow_stats:
    workflow_name = workflow['workflow']
    today_summary['workflows'][workflow_name] = {
        'tokens': workflow['total_tokens'],
        'cost': workflow['total_cost'],
        'runs': workflow['run_count'],
        'avg_tokens': workflow['avg_tokens'],
        'avg_cost': workflow['avg_cost']
    }
    total_tokens += workflow['total_tokens']
    total_cost += workflow['total_cost']
    total_runs += workflow['run_count']

today_summary['totals'] = {
    'tokens': total_tokens,
    'cost': total_cost,
    'runs': total_runs
}

# Ensure memory directory exists
memory_dir = '/tmp/gh-aw/repo-memory-default/memory/default'
os.makedirs(memory_dir, exist_ok=True)

# Append to history (JSON Lines format)
history_file = f'{memory_dir}/history.jsonl'
with open(history_file, 'a') as f:
    f.write(json.dumps(today_summary) + '\n')

print(f"‚úÖ Stored metrics for {today}")
print(f"üìà Total tokens: {total_tokens:,}")
print(f"üí∞ Total cost: ${total_cost:.2f}")
print(f"üîÑ Total runs: {total_runs}")
```

**IMPORTANT**: Copy the complete Python script from above (starting with `#!/usr/bin/env python3`) and save it to `/tmp/gh-aw/python/store_history.py`, then run it:

```bash
python3 /tmp/gh-aw/python/store_history.py
```

## Phase 3: Generate Trend Charts

### Step 3.1: Prepare Data for Visualization

Create CSV files for chart generation:

```python
#!/usr/bin/env python3
"""Prepare CSV data for trend charts"""
import json
import os
import pandas as pd
from datetime import datetime, timedelta

# Load historical data from repo memory
memory_dir = '/tmp/gh-aw/repo-memory-default/memory/default'
history_file = f'{memory_dir}/history.jsonl'

if not os.path.exists(history_file):
    print("‚ö†Ô∏è No historical data available yet. Charts will be generated from today's data only.")
    # Create a minimal dataset from today's data
    with open('/tmp/gh-aw/python/data/workflow_stats.json', 'r') as f:
        workflow_stats = json.load(f)
    
    # Create today's entry
    today = datetime.now().strftime('%Y-%m-%d')
    historical_data = [{
        'date': today,
        'totals': {
            'tokens': sum(w['total_tokens'] for w in workflow_stats),
            'cost': sum(w['total_cost'] for w in workflow_stats),
            'runs': sum(w['run_count'] for w in workflow_stats)
        }
    }]
else:
    # Load all historical data
    historical_data = []
    with open(history_file, 'r') as f:
        for line in f:
            if line.strip():
                historical_data.append(json.loads(line))

print(f"üìä Loaded {len(historical_data)} days of historical data")

# Prepare daily aggregates CSV
daily_data = []
for entry in historical_data:
    daily_data.append({
        'date': entry['date'],
        'tokens': entry['totals']['tokens'],
        'cost': entry['totals']['cost'],
        'runs': entry['totals']['runs']
    })

df_daily = pd.DataFrame(daily_data)
df_daily['date'] = pd.to_datetime(df_daily['date'])
df_daily = df_daily.sort_values('date')

# Save CSV for daily trends
os.makedirs('/tmp/gh-aw/python/data', exist_ok=True)
df_daily.to_csv('/tmp/gh-aw/python/data/daily_trends.csv', index=False)

print(f"‚úÖ Prepared daily trends CSV with {len(df_daily)} days")

# Prepare per-workflow trends CSV (last 30 days)
workflow_trends = []
for entry in historical_data:
    date = entry['date']
    for workflow_name, stats in entry.get('workflows', {}).items():
        workflow_trends.append({
            'date': date,
            'workflow': workflow_name,
            'tokens': stats['tokens'],
            'cost': stats['cost'],
            'runs': stats['runs']
        })

if workflow_trends:
    df_workflows = pd.DataFrame(workflow_trends)
    df_workflows['date'] = pd.to_datetime(df_workflows['date'])
    df_workflows = df_workflows.sort_values('date')
    df_workflows.to_csv('/tmp/gh-aw/python/data/workflow_trends.csv', index=False)
    print(f"‚úÖ Prepared workflow trends CSV with {len(df_workflows)} records")
```

**IMPORTANT**: Copy the complete Python script from above (starting with `#!/usr/bin/env python3`) and save it to `/tmp/gh-aw/python/prepare_charts.py`, then run it:

```bash
python3 /tmp/gh-aw/python/prepare_charts.py
```

### Step 3.2: Generate Trend Charts

Create high-quality visualizations:

```python
#!/usr/bin/env python3
"""Generate trend charts for token usage and costs"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Set style
sns.set_style("whitegrid")
sns.set_palette("husl")

# Ensure output directory exists
charts_dir = '/tmp/gh-aw/python/charts'
os.makedirs(charts_dir, exist_ok=True)

# Load daily trends
df_daily = pd.read_csv('/tmp/gh-aw/python/data/daily_trends.csv')
df_daily['date'] = pd.to_datetime(df_daily['date'])

print(f"Generating charts from {len(df_daily)} days of data...")

# Chart 1: Token Usage Over Time
fig, ax1 = plt.subplots(figsize=(12, 7), dpi=300)

color = 'tab:blue'
ax1.set_xlabel('Date', fontsize=12, fontweight='bold')
ax1.set_ylabel('Total Tokens', fontsize=12, fontweight='bold', color=color)
ax1.bar(df_daily['date'], df_daily['tokens'], color=color, alpha=0.6, label='Daily Tokens')
ax1.tick_params(axis='y', labelcolor=color)
ax1.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x/1000)}K' if x >= 1000 else str(int(x))))

# Add 7-day moving average if enough data
if len(df_daily) >= 7:
    df_daily['tokens_ma7'] = df_daily['tokens'].rolling(window=7, min_periods=1).mean()
    ax1.plot(df_daily['date'], df_daily['tokens_ma7'], color='darkblue', 
             linewidth=2.5, label='7-day Moving Avg', marker='o', markersize=4)

ax2 = ax1.twinx()
color = 'tab:orange'
ax2.set_ylabel('Number of Runs', fontsize=12, fontweight='bold', color=color)
ax2.plot(df_daily['date'], df_daily['runs'], color=color, linewidth=2, 
         label='Runs', marker='s', markersize=5)
ax2.tick_params(axis='y', labelcolor=color)

plt.title('Copilot Token Usage Trends', fontsize=16, fontweight='bold', pad=20)
fig.legend(loc='upper left', bbox_to_anchor=(0.1, 0.95), fontsize=10)
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(f'{charts_dir}/token_usage_trends.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("‚úÖ Generated token usage trends chart")

# Chart 2: Cost Trends Over Time
fig, ax = plt.subplots(figsize=(12, 7), dpi=300)

ax.bar(df_daily['date'], df_daily['cost'], color='tab:green', alpha=0.6, label='Daily Cost')

# Add 7-day moving average if enough data
if len(df_daily) >= 7:
    df_daily['cost_ma7'] = df_daily['cost'].rolling(window=7, min_periods=1).mean()
    ax.plot(df_daily['date'], df_daily['cost_ma7'], color='darkgreen', 
            linewidth=2.5, label='7-day Moving Avg', marker='o', markersize=4)

ax.set_xlabel('Date', fontsize=12, fontweight='bold')
ax.set_ylabel('Cost (USD)', fontsize=12, fontweight='bold')
ax.set_title('Copilot Token Cost Trends', fontsize=16, fontweight='bold', pad=20)
ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'${x:.2f}'))
ax.legend(loc='best', fontsize=10)
plt.xticks(rotation=45, ha='right')
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig(f'{charts_dir}/cost_trends.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("‚úÖ Generated cost trends chart")

# Chart 3: Top 10 Workflows by Token Usage
with open('/tmp/gh-aw/python/data/workflow_stats.json', 'r') as f:
    import json
    workflow_stats = json.load(f)

# Get top 10 by total tokens
top_workflows = sorted(workflow_stats, key=lambda x: x['total_tokens'], reverse=True)[:10]

fig, ax = plt.subplots(figsize=(12, 8), dpi=300)

workflows = [w['workflow'][:40] for w in top_workflows]  # Truncate long names
tokens = [w['total_tokens'] for w in top_workflows]
costs = [w['total_cost'] for w in top_workflows]

x = range(len(workflows))
width = 0.35

bars1 = ax.barh([i - width/2 for i in x], tokens, width, label='Tokens', color='tab:blue', alpha=0.7)
ax2 = ax.twiny()
bars2 = ax2.barh([i + width/2 for i in x], costs, width, label='Cost ($)', color='tab:orange', alpha=0.7)

ax.set_yticks(x)
ax.set_yticklabels(workflows, fontsize=9)
ax.set_xlabel('Total Tokens', fontsize=12, fontweight='bold', color='tab:blue')
ax2.set_xlabel('Total Cost (USD)', fontsize=12, fontweight='bold', color='tab:orange')
ax.tick_params(axis='x', labelcolor='tab:blue')
ax2.tick_params(axis='x', labelcolor='tab:orange')

plt.title('Top 10 Workflows by Token Consumption', fontsize=16, fontweight='bold', pad=40)
fig.legend(loc='lower right', bbox_to_anchor=(0.9, 0.05), fontsize=10)
plt.grid(True, alpha=0.3, axis='x')
plt.tight_layout()
plt.savefig(f'{charts_dir}/top_workflows.png', dpi=300, bbox_inches='tight', facecolor='white')
plt.close()

print("‚úÖ Generated top workflows chart")
print(f"\nüìà All charts saved to {charts_dir}/")
```

**IMPORTANT**: Copy the complete Python script from above (starting with `#!/usr/bin/env python3`) and save it to `/tmp/gh-aw/python/generate_charts.py`, then run it:

```bash
python3 /tmp/gh-aw/python/generate_charts.py
```

### Step 3.3: Upload Charts as Assets

Use the `upload asset` tool to upload the generated charts and collect URLs:

1. Upload `/tmp/gh-aw/python/charts/token_usage_trends.png`
2. Upload `/tmp/gh-aw/python/charts/cost_trends.png`
3. Upload `/tmp/gh-aw/python/charts/top_workflows.png`

Store the returned URLs for embedding in the report.

## Phase 4: Generate Report

Create a comprehensive discussion report with all findings.

**Note**: The report template below contains placeholder variables (e.g., `[DATE]`, `[TOTAL_TOKENS]`, `URL_FROM_UPLOAD_ASSET_CHART_1`) that you should replace with actual values during report generation.

### Report Structure

```markdown
# üìä Daily Copilot Token Consumption Report - [DATE]

### Executive Summary

Over the last 30 days, Copilot-powered agentic workflows consumed **[TOTAL_TOKENS]** tokens at an estimated cost of **$[TOTAL_COST]**, across **[TOTAL_RUNS]** workflow runs covering **[NUM_WORKFLOWS]** unique workflows.

#### Key Highlights:
- **Highest consuming workflow**: [WORKFLOW_NAME] ([TOKENS] tokens, $[COST])
- **Most active workflow**: [WORKFLOW_NAME] ([RUN_COUNT] runs)
- **Average cost per run**: $[AVG_COST]
- **Trend**: Token usage is [increasing/decreasing/stable] by [PERCENT]% over the last 7 days

### üìà Token Usage Trends

#### Overall Trends
![Token Usage Trends](URL_FROM_UPLOAD_ASSET_CHART_1)

The chart above shows daily token consumption over the last 30 days. [Brief analysis of the trend: are we increasing, decreasing, or stable? Any spikes or anomalies?]

#### Cost Trends
![Cost Trends](URL_FROM_UPLOAD_ASSET_CHART_2)

Daily cost trends show [analysis of cost patterns, efficiency, and notable changes].

### üèÜ Top Workflows by Token Consumption

![Top Workflows](URL_FROM_UPLOAD_ASSET_CHART_3)

#### Top 10 Most Expensive Workflows

| Rank | Workflow | Total Tokens | Total Cost | Runs | Avg Tokens/Run | Avg Cost/Run |
|------|----------|--------------|------------|------|----------------|--------------|
| 1    | [name]   | [tokens]     | $[cost]    | [n]  | [avg]          | $[avg]       |
| 2    | [name]   | [tokens]     | $[cost]    | [n]  | [avg]          | $[avg]       |
| ...  | ...      | ...          | ...        | ...  | ...            | ...          |

<details>
<summary><b>Per-Workflow Detailed Statistics (All Workflows)</b></summary>

| Workflow | Total Tokens | Total Cost | Runs | Avg Tokens | Avg Cost | Avg Turns | Avg Duration |
|----------|--------------|------------|------|------------|----------|-----------|--------------|
| [name]   | [tokens]     | $[cost]    | [n]  | [avg]      | $[avg]   | [turns]   | [duration]   |
| ...      | ...          | ...        | ...  | ...        | ...      | ...       | ...          |

</details>

### üí° Insights & Recommendations

#### High-Cost Workflows

The following workflows account for the majority of token consumption:

1. **[Workflow 1]** - $[cost] ([percent]% of total)
   - **Observation**: [Why is this workflow consuming so many tokens?]
   - **Recommendation**: [Specific optimization suggestion]

2. **[Workflow 2]** - $[cost] ([percent]% of total)
   - **Observation**: [Analysis]
   - **Recommendation**: [Suggestion]

<details>
<summary><b>Optimization Opportunities</b></summary>

1. **[Opportunity 1]**: [Description]
   - **Affected Workflows**: [list]
   - **Potential Savings**: ~$[amount] per month
   - **Action**: [Specific steps to implement]

2. **[Opportunity 2]**: [Description]
   - **Affected Workflows**: [list]
   - **Potential Savings**: ~$[amount] per month
   - **Action**: [Specific steps to implement]

</details>

<details>
<summary><b>Efficiency Trends</b></summary>

- **Token efficiency**: [Analysis of avg tokens per turn or per workflow]
- **Cost efficiency**: [Analysis of cost trends and efficiency improvements]
- **Run patterns**: [Any patterns in when workflows run or how often they succeed]

</details>

<details>
<summary><b>Historical Comparison</b></summary>

| Metric | Last 7 Days | Previous 7 Days | Change | Last 30 Days |
|--------|-------------|-----------------|--------|--------------|
| Total Tokens | [n] | [n] | [+/-]% | [n] |
| Total Cost | $[n] | $[n] | [+/-]% | $[n] |
| Total Runs | [n] | [n] | [+/-]% | [n] |
| Avg Cost/Run | $[n] | $[n] | [+/-]% | $[n] |

</details>

<details>
<summary><b>Methodology & Data Quality Notes</b></summary>

#### Methodology
- **Data Source**: GitHub Actions workflow run artifacts from last 30 days
- **Engine Filter**: Copilot engine only
- **Memory Storage**: `/tmp/gh-aw/repo-memory/default/`
- **Analysis Date**: [TIMESTAMP]
- **Historical Data**: [N] days of trend data
- **Cost Model**: Based on Copilot token pricing

#### Data Quality Notes
- [Any caveats about data completeness]
- [Note about workflows without cost data]
- [Any filtering or exclusions applied]

</details>

---

*Generated by Daily Copilot Token Consumption Report*
*Next report: Tomorrow at 11 AM UTC (weekdays only)*
```

## Important Guidelines

### Data Processing
- **Pre-downloaded logs**: Logs are already downloaded to `/tmp/gh-aw/copilot-logs.json` - use this file directly
- **Handle missing data**: Some runs may not have token usage data; skip or note these
- **Validate data**: Check for reasonable values before including in aggregates
- **Efficient processing**: Use bash and Python for data processing, avoid heavy operations

### Historical Tracking
- **Persistent storage**: Store daily aggregates in `/tmp/gh-aw/repo-memory/default/history.jsonl`
- **JSON Lines format**: One JSON object per line for efficient appending
- **Data retention**: Keep 90 days of history, prune older data
- **Recovery**: Handle missing or corrupted memory data gracefully

### Visualization
- **High-quality charts**: 300 DPI, 12x7 inch figures
- **Clear labels**: Bold titles, labeled axes, readable fonts
- **Multiple metrics**: Use dual y-axes to show related metrics
- **Trend lines**: Add moving averages for smoother trends
- **Professional styling**: Use seaborn for consistent, attractive charts

### Report Quality
- **Executive summary**: Start with high-level findings and key numbers
- **Visual first**: Lead with charts, then provide detailed tables
- **Actionable insights**: Focus on optimization opportunities and recommendations
- **Collapsible details**: Use `<details>` tags to keep report scannable
- **Historical context**: Always compare with previous periods

### Resource Efficiency
- **Batch operations**: Process all data in single passes
- **Cache results**: Store processed data to avoid recomputation
- **Timeout awareness**: Complete within 20-minute limit
- **Error handling**: Continue even if some workflows have incomplete data

## Success Criteria

A successful token consumption report:
- ‚úÖ Uses pre-downloaded logs from `/tmp/gh-aw/copilot-logs.json` (last 30 days)
- ‚úÖ Generates accurate per-workflow statistics
- ‚úÖ Stores daily aggregates in persistent repo memory
- ‚úÖ Creates 3 high-quality trend charts
- ‚úÖ Uploads charts as artifacts
- ‚úÖ Publishes comprehensive discussion report
- ‚úÖ Provides actionable optimization recommendations
- ‚úÖ Tracks trends over time with historical comparisons
- ‚úÖ Completes within timeout limits

## Output Requirements

Your output MUST:

1. Create a discussion in the "audits" category with the complete report
2. Include executive summary with key metrics and highlights
3. Embed all three generated charts with URLs from `upload asset` tool
4. Provide detailed per-workflow statistics in a table
5. Include trend analysis comparing recent periods
6. Offer specific optimization recommendations
7. Store current day's metrics in repo memory for future trend tracking
8. Use the collapsible details format from the reporting.md import

Begin your analysis now. The logs have been pre-downloaded to `/tmp/gh-aw/copilot-logs.json` - process the data systematically, generate insightful visualizations, and create a comprehensive report that helps optimize Copilot token consumption across all workflows.