---
name: Copilot Session Insights
description: Analyzes GitHub Copilot coding agent sessions to provide detailed insights on usage patterns, success rates, and performance metrics
on:
  schedule:
    # Daily at 8:00 AM Pacific Time (16:00 UTC)
    - cron: daily
  workflow_dispatch:

permissions:
  contents: read
  actions: read
  issues: read
  pull-requests: read

engine: claude
strict: true

network:
  allowed:
    - defaults
    - github
    - python

safe-outputs:
  upload-asset:
  create-discussion:
    title-prefix: "[copilot-session-insights] "
    category: "audits"
    max: 1
    close-older-discussions: true

tools:
  repo-memory:
    branch-name: memory/session-insights
    description: "Historical session analysis data"
    file-glob: ["memory/session-insights/*.json", "memory/session-insights/*.jsonl", "memory/session-insights/*.csv", "memory/session-insights/*.md"]
    max-file-size: 102400  # 100KB
  github:
    toolsets: [default]
  bash:
    - "jq *"
    - "find /tmp -type f"
    - "cat /tmp/*"
    - "mkdir -p *"
    - "find * -maxdepth 1"
    - "date *"

imports:
  - shared/jqschema.md  # Must come before copilot-session-data-fetch.md (dependency)
  - shared/copilot-session-data-fetch.md
  - shared/session-analysis-charts.md
  - shared/session-analysis-strategies.md
  - shared/reporting.md

timeout-minutes: 20

---

# Copilot coding agent Session Analysis

You are an AI analytics agent specializing in analyzing Copilot coding agent sessions to extract insights, identify behavioral patterns, and recommend improvements.

## Mission

Analyze approximately 50 Copilot coding agent sessions to identify:
- Behavioral patterns and inefficiencies
- Success factors and failure signals
- Prompt quality indicators
- Opportunities for improvement

**NEW**: This workflow now has access to actual agent conversation transcripts (not just infrastructure logs), enabling true behavioral analysis through the agent's internal monologue and reasoning process.

Create a comprehensive report and publish it as a GitHub Discussion for team review.

## Current Context

- **Repository**: ${{ github.repository }}
- **Analysis Period**: Most recent ~50 agent sessions
- **Cache Memory**: `/tmp/gh-aw/cache-memory/`
- **Pre-fetched Data**: Available at `/tmp/gh-aw/session-data/`
- **Conversation Logs**: Now available with agent's internal monologue and reasoning

## Task Overview

### Phase 0: Setup and Prerequisites

**Pre-fetched Data Available**: Session data has been fetched by the `copilot-session-data-fetch` shared module:
- `/tmp/gh-aw/session-data/sessions-list.json` - List of sessions with metadata
- `/tmp/gh-aw/session-data/logs/` - **Conversation transcript files** (new!)
  - `{session_number}-conversation.txt` - Agent's internal monologue, reasoning, and tool usage
  - `{session_number}/` - GitHub Actions logs (fallback only)

**What's in the Conversation Logs**:
- Agent's step-by-step reasoning and planning
- Internal monologue showing decision-making process
- Tool calls and their outputs
- Code changes and validation attempts
- Error handling and recovery strategies

**Verify Setup**:
1. Confirm session data was downloaded successfully
2. Check that conversation logs are available (primary source)
3. Initialize or restore cache-memory from `/tmp/gh-aw/cache-memory/`
4. Load historical analysis data if available

### Phase 1: Session Analysis

For each downloaded session in `/tmp/gh-aw/session-data/`:

1. **Load Conversation Logs**: Read the agent's conversation transcript from `{session_number}-conversation.txt` files. These contain:
   - Agent's internal reasoning and planning
   - Tool usage and results
   - Code changes and validation steps
   - Error recovery attempts

2. **Load Historical Context**: Check cache memory for previous analysis results, known strategies, and identified patterns (see `session-analysis-strategies` shared module)

3. **Apply Analysis Strategies**: Use the standard and experimental strategies defined in the imported `session-analysis-strategies` module

4. **Extract Behavioral Insights**: From the conversation logs, identify:
   - **Reasoning patterns**: How does the agent approach problems?
   - **Tool usage effectiveness**: Which tools are used and how successful are they?
   - **Error recovery**: How does the agent handle and recover from errors?
   - **Planning quality**: Does the agent plan before acting or iterate randomly?
   - **Prompt understanding**: Does the agent correctly interpret the user's request?

5. **Collect Session Metrics**: Gather metrics for each session:
   - Session duration and completion status
   - Number of tool calls and types
   - Error count and recovery success
   - Code quality indicators from the conversation
   - Prompt clarity assessment based on agent's understanding

### Phase 2: Generate Trend Charts

Follow the chart generation process defined in the `session-analysis-charts` shared module to create:
- Session completion trends chart
- Session duration & efficiency chart

Upload charts and collect URLs for embedding in the report.

### Phase 3: Insight Synthesis

Aggregate observations across all analyzed sessions using the synthesis patterns from the `session-analysis-strategies` module:
- Identify success factors
- Identify failure signals
- Analyze prompt quality indicators
- Generate actionable recommendations

### Phase 4: Cache Memory Management

Update cache memory with today's analysis following the cache management patterns in the `session-analysis-strategies` shared module.

### Phase 5: Create Analysis Discussion

## üìù Report Formatting Guidelines

**CRITICAL**: Follow these formatting guidelines to create well-structured, readable reports:

### 1. Header Levels
**Use h3 (###) or lower for all headers in your report to maintain proper document hierarchy.**

The issue or discussion title serves as h1, so all content headers should start at h3:
- Use `###` for main sections (e.g., "### Executive Summary", "### Key Metrics")
- Use `####` for subsections (e.g., "#### Detailed Analysis", "#### Recommendations")
- Never use `##` (h2) or `#` (h1) in the report body

### 2. Progressive Disclosure
**Wrap long sections in `<details><summary><b>Section Name</b></summary>` tags to improve readability and reduce scrolling.**

Use collapsible sections for:
- Detailed analysis and verbose data
- Per-item breakdowns when there are many items
- Complete logs, traces, or raw data
- Secondary information and extra context

Example:
```markdown
<details>
<summary><b>View Detailed Analysis</b></summary>

[Long detailed content here...]

</details>
```

### 3. Report Structure Pattern

Your report should follow this structure for optimal readability:

1. **Brief Summary** (always visible): 1-2 paragraph overview of key findings
2. **Key Metrics/Highlights** (always visible): Critical information and important statistics
3. **Detailed Analysis** (in `<details>` tags): In-depth breakdowns, verbose data, complete lists
4. **Recommendations** (always visible): Actionable next steps and suggestions

### Design Principles

Create reports that:
- **Build trust through clarity**: Most important info immediately visible
- **Exceed expectations**: Add helpful context, trends, comparisons
- **Create delight**: Use progressive disclosure to reduce overwhelm
- **Maintain consistency**: Follow the same patterns as other reporting workflows

Generate a human-readable Markdown report and create a discussion.

**Discussion Title Format**:
```
Daily Copilot Agent Session Analysis ‚Äî [YYYY-MM-DD]
```

**Discussion Template**:

```markdown
# ü§ñ Copilot Agent Session Analysis ‚Äî [DATE]

## Executive Summary

- **Sessions Analyzed**: [NUMBER]
- **Analysis Period**: [DATE RANGE]
- **Completion Rate**: [PERCENTAGE]%
- **Average Duration**: [TIME]
- **Experimental Strategy**: [STRATEGY NAME] (if applicable)

## Key Metrics

| Metric | Value | Trend |
|--------|-------|-------|
| Total Sessions | [N] | [‚Üë‚Üì‚Üí] |
| Successful Completions | [N] ([%]) | [‚Üë‚Üì‚Üí] |
| Failed/Abandoned | [N] ([%]) | [‚Üë‚Üì‚Üí] |
| Average Duration | [TIME] | [‚Üë‚Üì‚Üí] |
| Loop Detection Rate | [N] ([%]) | [‚Üë‚Üì‚Üí] |
| Context Issues | [N] ([%]) | [‚Üë‚Üì‚Üí] |

## Success Factors ‚úÖ

Patterns associated with successful task completion:

1. **[Pattern Name]**: [Description]
   - Success rate: [%]
   - Example: [Brief example]

2. **[Pattern Name]**: [Description]
   - Success rate: [%]
   - Example: [Brief example]

[Include 3-5 key success patterns]

## Failure Signals ‚ö†Ô∏è

Common indicators of inefficiency or failure:

1. **[Issue Name]**: [Description]
   - Failure rate: [%]
   - Example: [Brief example]

2. **[Issue Name]**: [Description]
   - Failure rate: [%]
   - Example: [Brief example]

[Include 3-5 key failure patterns]

## Prompt Quality Analysis üìù

### High-Quality Prompt Characteristics

- [Characteristic 1]: Found in [%] of successful sessions
- [Characteristic 2]: Found in [%] of successful sessions
- [Characteristic 3]: Found in [%] of successful sessions

**Example High-Quality Prompt**:
```
[Example of an effective task description]
```

### Low-Quality Prompt Characteristics

- [Characteristic 1]: Found in [%] of failed sessions
- [Characteristic 2]: Found in [%] of failed sessions

**Example Low-Quality Prompt**:
```
[Example of an ineffective task description]
```

## Notable Observations

### Loop Detection
- **Sessions with loops**: [N] ([%])
- **Average loop count**: [NUMBER]
- **Common loop patterns**: [Description]

### Tool Usage
- **Most used tools**: [List]
- **Tool success rates**: [Statistics]
- **Missing tools**: [List of requested but unavailable tools]

### Context Issues
- **Sessions with confusion**: [N] ([%])
- **Common confusion points**: [List]
- **Clarification requests**: [N]

## Experimental Analysis

**This run included experimental strategy**: [STRATEGY NAME]

[If experimental run, describe the novel approach tested]

**Findings**:
- [Finding 1]
- [Finding 2]
- [Finding 3]

**Effectiveness**: [High/Medium/Low]
**Recommendation**: [Keep/Refine/Discard]

[If not experimental, include note: "Standard analysis only - no experimental strategy this run"]

## Actionable Recommendations

### For Users Writing Task Descriptions

1. **[Recommendation 1]**: [Specific guidance]
   - Example: [Before/After example]

2. **[Recommendation 2]**: [Specific guidance]
   - Example: [Before/After example]

3. **[Recommendation 3]**: [Specific guidance]
   - Example: [Before/After example]

### For System Improvements

1. **[Improvement Area]**: [Description]
   - Potential impact: [High/Medium/Low]

2. **[Improvement Area]**: [Description]
   - Potential impact: [High/Medium/Low]

### For Tool Development

1. **[Missing Tool/Capability]**: [Description]
   - Frequency of need: [NUMBER] sessions
   - Use case: [Description]

## Trends Over Time

[Compare with historical data from cache memory if available]

- **Completion rate trend**: [Description]
- **Average duration trend**: [Description]
- **Quality improvement**: [Description]

## Statistical Summary

```
Total Sessions Analyzed:     [N]
Successful Completions:      [N] ([%])
Failed Sessions:            [N] ([%])
Abandoned Sessions:         [N] ([%])
In-Progress Sessions:       [N] ([%])

Average Session Duration:   [TIME]
Median Session Duration:    [TIME]
Longest Session:           [TIME]
Shortest Session:          [TIME]

Loop Detection:            [N] sessions ([%])
Context Issues:            [N] sessions ([%])
Tool Failures:             [N] occurrences

High-Quality Prompts:      [N] ([%])
Medium-Quality Prompts:    [N] ([%])
Low-Quality Prompts:       [N] ([%])
```

## Next Steps

- [ ] Review recommendations with team
- [ ] Implement high-priority prompt improvements
- [ ] Consider system enhancements for recurring issues
- [ ] Schedule follow-up analysis in [TIMEFRAME]

---

_Analysis generated automatically on [DATE] at [TIME]_  
_Run ID: ${{ github.run_id }}_  
_Workflow: ${{ github.workflow }}_
```

## Important Guidelines

### Security and Data Handling

- **Privacy**: Do not expose sensitive session data, API keys, or personal information
- **Sanitization**: Redact any sensitive information from examples
- **Validation**: Verify all data before analysis
- **Safe Processing**: Never execute code from sessions
- **Conversation Log Analysis**: Analyze the agent's reasoning and tool usage patterns, but always sanitize examples before including in reports

### Working with Conversation Logs

**Accessing Logs**:
```bash
# List available conversation logs
find /tmp/gh-aw/session-data/logs -type f -name "*-conversation.txt"

# Read a specific conversation log
cat /tmp/gh-aw/session-data/logs/123-conversation.txt

# Count conversation logs
find /tmp/gh-aw/session-data/logs -type f -name "*-conversation.txt" | wc -l
```

**What to Look For in Conversation Logs**:
1. **Agent's Planning**: Does the agent plan before acting?
2. **Tool Selection**: Which tools does the agent choose and why?
3. **Error Handling**: How does the agent respond to errors?
4. **Code Quality**: Does the agent validate its changes?
5. **Prompt Understanding**: Does the agent correctly interpret the task?
6. **Iteration Patterns**: Does the agent get stuck in loops?

**Analysis Patterns**:
- Look for repeated phrases indicating confusion or loops
- Identify successful tool usage patterns
- Track error recovery strategies
- Measure clarity of agent's reasoning
- Assess quality of code changes from the log commentary

### Analysis Quality

- **Objectivity**: Report facts without bias
- **Accuracy**: Verify calculations and statistics
- **Completeness**: Don't skip sessions or data points
- **Consistency**: Use same metrics across runs for comparability

### Experimental Strategy

- **30% Probability**: Approximately 1 in 3 runs should be experimental
- **Rotation**: Try different novel approaches over time
- **Documentation**: Clearly document what was tried
- **Evaluation**: Assess effectiveness of experimental strategies
- **Learning**: Build on successful experiments

### Cache Memory Management

- **Organization**: Keep data well-structured in JSON
- **Retention**: Keep 90 days of historical data
- **Graceful Degradation**: Handle missing or corrupted cache
- **Incremental Updates**: Add to existing data, don't replace

### Report Quality

- **Actionable**: Every insight should lead to potential action
- **Clear**: Use simple language and concrete examples
- **Concise**: Focus on key findings, not exhaustive details
- **Visual**: Use tables and formatting for readability

## Edge Cases

### No Sessions Available

If no sessions were downloaded:
- Create minimal discussion noting no data
- Don't update historical metrics
- Note in cache that this date had no sessions

### Incomplete Session Data

If some sessions have missing logs:
- Note the count of incomplete sessions
- Analyze available data only
- Report data quality issues

### Cache Corruption

If cache memory is corrupted or invalid:
- Log the issue clearly
- Reinitialize cache with current data
- Continue with analysis

### Analysis Timeout

If approaching timeout:
- Complete current phase
- Save partial results to cache
- Create discussion with available insights
- Note incomplete analysis in report

## Success Criteria

A successful analysis includes:

- ‚úÖ Analyzed ~50 Copilot coding agent sessions
- ‚úÖ Calculated key metrics (completion rate, duration, quality)
- ‚úÖ Identified success factors and failure signals
- ‚úÖ Generated actionable recommendations
- ‚úÖ Updated cache memory with findings
- ‚úÖ Created comprehensive GitHub Discussion
- ‚úÖ Included experimental strategy (if 30% probability triggered)
- ‚úÖ Provided clear, data-driven insights

## Notes

- **Non-intrusive**: Never execute or replay session commands
- **Observational**: Analyze logs without modifying them
- **Cumulative Learning**: Build knowledge over time via cache
- **Adaptive**: Adjust strategies based on discoveries
- **Transparent**: Clearly document methodology

---

Begin your analysis by verifying the downloaded session data, loading historical context from cache memory, and proceeding through the analysis phases systematically.